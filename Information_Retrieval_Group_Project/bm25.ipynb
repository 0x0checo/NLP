{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7da68ec-719d-4d98-b506-9bb93047a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 19:14:18 INFO: Loading these models for language: hr (Croatian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "| lemma     | standard |\n",
      "========================\n",
      "\n",
      "2025-05-27 19:14:18 INFO: Use device: cpu\n",
      "2025-05-27 19:14:18 INFO: Loading: tokenize\n",
      "2025-05-27 19:14:18 INFO: Loading: pos\n",
      "2025-05-27 19:14:24 INFO: Loading: lemma\n",
      "2025-05-27 19:14:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correct doc in top 5 for 2020/2109 queries.\n",
      "Accuracy@5: 95.78%\n",
      "Average rank position (for successful hits): 1.31\n",
      "Mean Average Precision (MAP): 0.8466\n",
      "Similarity score usefulness: 0.0001\n",
      "[9, 45, 92, 125, 134, 141, 178, 187, 206, 265, 315, 351, 380, 391, 394, 396, 401, 415, 533, 576, 594, 662, 666, 675, 676, 681, 682, 722, 723, 748, 768, 783, 791, 891, 902, 903, 908, 923, 926, 931, 941, 1002, 1008, 1030, 1034, 1052, 1059, 1081, 1116, 1137, 1162, 1175, 1197, 1198, 1284, 1317, 1354, 1375, 1383, 1384, 1391, 1413, 1472, 1493, 1498, 1517, 1536, 1549, 1556, 1564, 1587, 1637, 1679, 1689, 1719, 1770, 1871, 1872, 1873, 1875, 1897, 1919, 1951, 1992, 2032, 2039, 2042, 2081, 2098]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import classla\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 初始化CLASSLA管道\n",
    "nlp = classla.Pipeline(lang='hr', processors='tokenize, pos, lemma')\n",
    "\n",
    "# 加载数据并构建ID到标题的字典\n",
    "x = pd.read_json('individual_data.json')\n",
    "new_dict = dict(zip(x['id'], x['title']))\n",
    "\n",
    "# 加载停用词\n",
    "with open('stopwords-hr.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "# 清理和词形还原函数\n",
    "def clean_lemmatize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            lemma = word.lemma.lower().strip()\n",
    "            if lemma and lemma not in string.punctuation and lemma not in stopwords:\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# 加载或生成词形还原数据\n",
    "lemmatized_file = 'lemmatized_data.pkl'\n",
    "if os.path.exists(lemmatized_file):\n",
    "    df = pd.read_pickle(lemmatized_file)\n",
    "else:\n",
    "    df = pd.read_json('individual_data.json')\n",
    "    df = df.drop('title', axis=1)\n",
    "    df['body'] = df['body'].apply(clean_lemmatize)\n",
    "    df.to_pickle(lemmatized_file)\n",
    "\n",
    "# 创建词汇表\n",
    "vocab = set()\n",
    "for body in df['body']:\n",
    "    for word in body:\n",
    "        vocab.add(word)\n",
    "vocablist = list(vocab)\n",
    "\n",
    "# 构建词-文档矩阵\n",
    "def term_document_matrix(data, vocab, document_index='id', text='body'):\n",
    "    if document_index not in data.columns:\n",
    "        raise ValueError(f\"Column '{document_index}' not found in data\")\n",
    "    vocab_index = pd.DataFrame(0, index=vocab, columns=data[document_index])\n",
    "    for doc_id, lemmas in zip(data[document_index], data[text]):\n",
    "        counts = Counter(lemmas)\n",
    "        for lemma, freq in counts.items():\n",
    "            if lemma in vocab_index.index:\n",
    "                vocab_index.at[lemma, doc_id] = freq\n",
    "    return vocab_index\n",
    "\n",
    "term_doc_matrix = term_document_matrix(df, vocablist, document_index='id', text='body')\n",
    "\n",
    "# 计算IDF\n",
    "document_index = df.id.values\n",
    "doc_freq = (term_doc_matrix[document_index] > 0).sum(axis=1)\n",
    "idf_series = np.log2(len(document_index) / doc_freq.replace(0, 1))\n",
    "\n",
    "# 查询处理\n",
    "def query_processing(query):\n",
    "    if not isinstance(query, str):\n",
    "        return []\n",
    "    query = re.sub(r'\\W+', ' ', query).strip().lower()\n",
    "    doc = nlp(query)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            lemma = word.lemma.lower().strip()\n",
    "            if lemma and lemma not in string.punctuation and lemma not in stopwords:\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# BM25评分函数\n",
    "def bm25_score(term_doc_matrix, query_lemmas, idf_series, document_index, k1=1.5, b=0.75):\n",
    "    doc_lengths = term_doc_matrix[document_index].sum(axis=0)\n",
    "    avgdl = doc_lengths.mean()\n",
    "    scores = pd.Series(0.0, index=document_index)\n",
    "    for term in set(query_lemmas):\n",
    "        if term not in term_doc_matrix.index:\n",
    "            continue\n",
    "        tf = term_doc_matrix.loc[term, document_index]\n",
    "        idf = idf_series.get(term, 0)\n",
    "        numerator = tf * (k1 + 1)\n",
    "        denominator = tf + k1 * (1 - b + b * doc_lengths / avgdl)\n",
    "        score = idf * numerator / (denominator + 1e-10)\n",
    "        scores += score\n",
    "    return scores.sort_values(ascending=False)\n",
    "\n",
    "# 检索文档\n",
    "def retrieve_index(data, scores, document_index):\n",
    "    data = data.set_index(document_index)\n",
    "    data['scores'] = scores\n",
    "    top_ids = data.sort_values('scores', ascending=False).head(5).index\n",
    "    return top_ids.tolist()\n",
    "\n",
    "counter = 0\n",
    "total = len(new_dict)\n",
    "ranks = []\n",
    "average_precisions = []\n",
    "errors = []\n",
    "\n",
    "fours_fives = pd.read_json('changed_4_and_5.json')\n",
    "\n",
    "total_fourfive = 0\n",
    "fourfive_count = 0\n",
    "\n",
    "for doc_id, title in new_dict.items():\n",
    "    # Step 1: Process the title (query)\n",
    "    qlemmas = query_processing(title)\n",
    "\n",
    "    # Step 2: Compute BM25 scores\n",
    "    scores = bm25_score(term_doc_matrix, qlemmas, idf_series, document_index)\n",
    "\n",
    "    # Step 3: Get top 5 doc IDs (ranked)\n",
    "    sorted_doc_ids = scores.sort_values(ascending=False).index.tolist()\n",
    "    top_doc_ids = sorted_doc_ids[:5]\n",
    "\n",
    "    # Accuracy@5 and Rank tracking\n",
    "    if doc_id in top_doc_ids:\n",
    "        counter += 1\n",
    "        rank = top_doc_ids.index(doc_id) + 1\n",
    "        ranks.append(rank)\n",
    "        average_precisions.append(1 / rank)  # AP for this query\n",
    "    else:\n",
    "        errors.append(doc_id)\n",
    "        average_precisions.append(0.0)\n",
    "\n",
    "    # Usefulness check for 4s and 5s\n",
    "    for doc_id_candidate in top_doc_ids:\n",
    "        for _, d in fours_fives.iterrows():\n",
    "            total_fourfive += 1\n",
    "            if doc_id_candidate == d[\"id\"] and d[\"id2\"] not in top_doc_ids:\n",
    "                fourfive_count += 1\n",
    "\n",
    "# Final metrics\n",
    "accuracy = counter / total\n",
    "avg_rank = sum(ranks) / len(ranks) if ranks else None\n",
    "map_score = sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
    "usefulness = fourfive_count / total_fourfive if total_fourfive > 0 else 0.0\n",
    "\n",
    "# Output\n",
    "print(f\"Found correct doc in top 5 for {counter}/{total} queries.\")\n",
    "print(f\"Accuracy@5: {accuracy:.2%}\")\n",
    "if avg_rank is not None:\n",
    "    print(f\"Average rank position (for successful hits): {avg_rank:.2f}\")\n",
    "else:\n",
    "    print(\"No correct documents found in top 5; cannot compute average rank.\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")\n",
    "print(f\"Similarity score usefulness: {usefulness:.4f}\")\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aaba36-3845-45ec-8b00-f90e90493230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
