{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2642c65-d964-41e3-8a72-cd1d3d3525c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:38:22 INFO: Loading these models for language: hr (Croatian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "| lemma     | standard |\n",
      "========================\n",
      "\n",
      "2025-05-28 11:38:22 INFO: Use device: cpu\n",
      "2025-05-28 11:38:22 INFO: Loading: tokenize\n",
      "2025-05-28 11:38:22 INFO: Loading: pos\n",
      "2025-05-28 11:38:23 INFO: Loading: lemma\n",
      "2025-05-28 11:38:34 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached lemmatized data...\n",
      "\n",
      "Found correct doc in top 5 for 1362/2109 queries.\n",
      "Accuracy@5: 64.58%\n",
      "Average rank position (for successful hits): 1.71\n",
      "Mean Average Precision (MAP): 0.5009\n",
      "Similarity score usefulness: 0.0003\n",
      "[1, 2, 3, 7, 8, 9, 11, 18, 24, 27, 28, 33, 37, 38, 39, 40, 41, 43, 45, 46, 56, 60, 62, 66, 74, 76, 91, 92, 93, 95, 98, 105, 106, 116, 118, 121, 125, 127, 131, 132, 133, 134, 137, 140, 141, 142, 145, 147, 148, 151, 155, 156, 157, 158, 161, 166, 168, 170, 171, 173, 175, 178, 183, 186, 187, 195, 196, 198, 199, 204, 205, 207, 208, 210, 211, 212, 216, 222, 229, 234, 235, 238, 240, 246, 247, 248, 249, 252, 253, 256, 257, 259, 260, 262, 265, 267, 269, 273, 275, 277, 279, 283, 284, 286, 291, 294, 295, 299, 300, 302, 304, 305, 307, 308, 313, 315, 317, 320, 321, 322, 326, 334, 337, 339, 346, 347, 351, 357, 359, 366, 368, 369, 370, 371, 379, 381, 383, 384, 385, 386, 387, 391, 392, 394, 401, 405, 408, 409, 410, 412, 415, 417, 419, 422, 423, 427, 432, 434, 437, 438, 443, 445, 446, 448, 449, 470, 472, 477, 478, 479, 481, 482, 485, 487, 488, 489, 493, 495, 496, 500, 501, 502, 507, 513, 515, 516, 517, 518, 522, 525, 527, 530, 531, 533, 534, 537, 538, 539, 543, 546, 548, 549, 552, 553, 555, 556, 558, 561, 562, 566, 567, 568, 569, 571, 576, 581, 582, 583, 587, 593, 596, 597, 606, 612, 613, 619, 626, 628, 634, 644, 649, 650, 652, 656, 657, 660, 661, 662, 666, 667, 668, 669, 671, 672, 673, 675, 676, 681, 682, 683, 686, 687, 706, 707, 708, 716, 722, 723, 731, 732, 736, 737, 738, 739, 740, 742, 744, 748, 751, 754, 757, 759, 763, 764, 766, 767, 768, 771, 774, 776, 781, 783, 785, 786, 787, 789, 790, 791, 792, 794, 795, 797, 798, 799, 802, 810, 817, 820, 821, 823, 824, 834, 837, 846, 848, 849, 866, 867, 874, 876, 878, 880, 881, 884, 889, 891, 895, 899, 902, 903, 905, 906, 908, 910, 912, 914, 915, 918, 923, 926, 931, 935, 936, 941, 942, 943, 946, 949, 954, 955, 956, 957, 963, 970, 972, 974, 975, 977, 978, 980, 981, 983, 985, 986, 988, 990, 998, 1004, 1008, 1030, 1034, 1035, 1038, 1039, 1041, 1043, 1045, 1050, 1051, 1052, 1057, 1059, 1060, 1064, 1066, 1072, 1075, 1076, 1077, 1078, 1081, 1083, 1084, 1091, 1092, 1096, 1103, 1107, 1108, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1121, 1123, 1126, 1127, 1129, 1133, 1134, 1137, 1139, 1140, 1142, 1150, 1152, 1156, 1157, 1162, 1165, 1167, 1170, 1175, 1177, 1178, 1181, 1182, 1188, 1198, 1204, 1207, 1211, 1212, 1213, 1214, 1215, 1220, 1222, 1223, 1225, 1226, 1230, 1231, 1233, 1234, 1238, 1241, 1243, 1246, 1256, 1258, 1261, 1262, 1264, 1265, 1266, 1272, 1273, 1278, 1280, 1298, 1305, 1310, 1311, 1312, 1323, 1327, 1330, 1334, 1339, 1340, 1341, 1343, 1344, 1348, 1356, 1361, 1371, 1372, 1373, 1374, 1375, 1379, 1380, 1385, 1386, 1387, 1391, 1396, 1398, 1403, 1407, 1411, 1412, 1413, 1414, 1425, 1431, 1436, 1437, 1439, 1442, 1443, 1444, 1445, 1446, 1448, 1449, 1450, 1464, 1465, 1470, 1471, 1472, 1476, 1483, 1484, 1488, 1490, 1492, 1493, 1495, 1496, 1497, 1498, 1499, 1501, 1503, 1505, 1507, 1508, 1510, 1511, 1519, 1523, 1534, 1536, 1540, 1542, 1545, 1546, 1547, 1548, 1551, 1553, 1555, 1562, 1563, 1564, 1566, 1569, 1572, 1573, 1581, 1583, 1587, 1595, 1597, 1603, 1608, 1614, 1615, 1623, 1625, 1626, 1628, 1631, 1635, 1636, 1637, 1642, 1647, 1649, 1651, 1656, 1657, 1658, 1660, 1662, 1664, 1665, 1666, 1669, 1670, 1675, 1676, 1679, 1680, 1684, 1687, 1689, 1693, 1696, 1701, 1703, 1704, 1706, 1708, 1710, 1712, 1715, 1719, 1721, 1724, 1727, 1733, 1738, 1740, 1741, 1748, 1756, 1757, 1758, 1761, 1762, 1764, 1767, 1770, 1775, 1777, 1786, 1787, 1788, 1789, 1791, 1792, 1794, 1795, 1796, 1798, 1812, 1815, 1817, 1819, 1820, 1821, 1822, 1823, 1826, 1830, 1832, 1834, 1836, 1837, 1838, 1839, 1842, 1847, 1853, 1857, 1864, 1869, 1870, 1871, 1872, 1874, 1877, 1882, 1885, 1886, 1888, 1891, 1893, 1894, 1896, 1897, 1898, 1900, 1901, 1902, 1903, 1912, 1914, 1915, 1918, 1919, 1921, 1924, 1926, 1927, 1928, 1931, 1935, 1936, 1937, 1939, 1941, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1951, 1954, 1960, 1971, 1973, 1975, 1979, 1985, 1987, 1993, 1994, 1996, 2004, 2005, 2006, 2007, 2010, 2012, 2013, 2016, 2018, 2020, 2024, 2028, 2029, 2031, 2032, 2033, 2035, 2039, 2041, 2043, 2044, 2049, 2050, 2051, 2059, 2060, 2065, 2068, 2069, 2072, 2074, 2075, 2078, 2079, 2081, 2085, 2088, 2089, 2092, 2094, 2096, 2097, 2098, 2101, 2103, 2104, 2106, 2108]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import classla\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load and preprocess data\n",
    "x = pd.read_json('individual_data.json')\n",
    "new_dict = dict(zip(x['id'], x['title']))\n",
    "\n",
    "# Initialize CLASSLA pipeline for Croatian\n",
    "nlp = classla.Pipeline(lang='hr', processors='tokenize,pos,lemma')\n",
    "\n",
    "# Load Croatian stopwords\n",
    "with open('stopwords-hr.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "def clean_lemmatize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            lemma = word.lemma.lower().strip()\n",
    "            if lemma and lemma not in string.punctuation and lemma not in stopwords:\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Load or create lemmatized data\n",
    "lemmatized_file = 'lemmatized_data.pkl'\n",
    "if os.path.exists(lemmatized_file):\n",
    "    print(\"Loading cached lemmatized data...\")\n",
    "    df = pd.read_pickle(lemmatized_file)\n",
    "else:\n",
    "    print(\"Lemmatizing data from scratch...\")\n",
    "    df = pd.read_json('individual_data.json')\n",
    "    df = df.drop('title', axis=1)\n",
    "    df['body'] = df['body'].apply(clean_lemmatize)\n",
    "    df.to_pickle(lemmatized_file)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for body in df['body']:\n",
    "    vocab.update(body)\n",
    "vocablist = list(vocab)\n",
    "\n",
    "# Build term-document matrix\n",
    "def term_document_matrix(data, vocab, document_index='id', text='body'):\n",
    "    if document_index not in data.columns:\n",
    "        raise ValueError(f\"Column '{document_index}' not found in data\")\n",
    "    vocab_index = pd.DataFrame(0, index=vocab, columns=data[document_index])\n",
    "    for doc_id, lemmas in zip(data[document_index], data[text]):\n",
    "        counts = Counter(lemmas)\n",
    "        for lemma, freq in counts.items():\n",
    "            if lemma in vocab_index.index:\n",
    "                vocab_index.at[lemma, doc_id] = freq\n",
    "    return vocab_index\n",
    "\n",
    "term_doc_matrix = term_document_matrix(df, vocablist, document_index='id', text='body')\n",
    "document_ids = df['id'].values\n",
    "doc_lengths = term_doc_matrix[document_ids].sum(axis=0)\n",
    "avgdl = doc_lengths.mean()\n",
    "\n",
    "# Compute IDF\n",
    "doc_freq = (term_doc_matrix[document_ids] > 0).sum(axis=1)\n",
    "idf_series = np.log2(len(document_ids) / doc_freq.replace(0, 1))\n",
    "\n",
    "# Query processing\n",
    "def query_processing(query):\n",
    "    if not isinstance(query, str):\n",
    "        return []\n",
    "    query = re.sub(r'\\W+', ' ', query).strip().lower()\n",
    "    doc = nlp(query)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            lemma = word.lemma.lower().strip()\n",
    "            if lemma and lemma not in string.punctuation and lemma not in stopwords:\n",
    "                lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Hardcoded BM25 retrieval\n",
    "def bm25_score(term_doc_matrix, query_lemmas, idf_series, document_ids, k1=1.5, b=0.75):\n",
    "    scores = pd.Series(0.0, index=document_ids)\n",
    "    for term in set(query_lemmas):\n",
    "        if term not in term_doc_matrix.index:\n",
    "            continue\n",
    "        tf = term_doc_matrix.loc[term, document_ids]\n",
    "        idf = idf_series.get(term, 0)\n",
    "        numerator = tf * (k1 + 1)\n",
    "        denominator = tf + k1 * (1 - b + b * doc_lengths / avgdl)\n",
    "        score = idf * numerator / (denominator + 1e-10)\n",
    "        scores += score\n",
    "    return scores.sort_values(ascending=False)\n",
    "\n",
    "def bm25_retrieval(query_lemmas, term_doc_matrix, idf_series, document_ids, top_k=100):\n",
    "    scores = bm25_score(term_doc_matrix, query_lemmas, idf_series, document_ids)\n",
    "    top_doc_ids = scores.head(top_k).index.tolist()\n",
    "    return top_doc_ids, scores.loc[top_doc_ids].values\n",
    "\n",
    "# SBERT reranking\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "original_texts = pd.read_json('individual_data.json').set_index('id')['body'].to_dict()\n",
    "\n",
    "def sbert_rerank(query, doc_ids, original_texts, sbert_model, top_k=5):\n",
    "    doc_texts = [original_texts.get(doc_id, \"\") for doc_id in doc_ids]\n",
    "\n",
    "    valid_doc_ids = []\n",
    "    valid_texts = []\n",
    "    for doc_id, text in zip(doc_ids, doc_texts):\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            valid_doc_ids.append(doc_id)\n",
    "            valid_texts.append(text)\n",
    "\n",
    "    if not valid_texts:\n",
    "        return []\n",
    "\n",
    "    query_embedding = sbert_model.encode(query, convert_to_tensor=True)\n",
    "    doc_embeddings = sbert_model.encode(valid_texts, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=min(top_k, len(valid_doc_ids)))\n",
    "    return [valid_doc_ids[i] for i in top_results.indices.tolist()]\n",
    "\n",
    "# Evaluation\n",
    "def average_precision_single_relevant(retrieved, relevant_doc_id):\n",
    "    if relevant_doc_id in retrieved:\n",
    "        rank = retrieved.index(relevant_doc_id) + 1\n",
    "        return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "counter = 0\n",
    "total = len(new_dict)\n",
    "ranks = []\n",
    "total_ap = 0.0\n",
    "errors = []\n",
    "\n",
    "fours_fives = pd.read_json('changed_4_and_5.json')\n",
    "total_fourfive = 0\n",
    "fourfive_count = 0\n",
    "\n",
    "for doc_id, title in new_dict.items():\n",
    "    qlemmas = query_processing(title)\n",
    "    top_doc_ids, _ = bm25_retrieval(qlemmas, term_doc_matrix, idf_series, document_ids, top_k=100)\n",
    "    reranked_doc_ids = sbert_rerank(title, top_doc_ids, original_texts, sbert_model, top_k=5)\n",
    "\n",
    "    if doc_id in reranked_doc_ids:\n",
    "        counter += 1\n",
    "        rank = reranked_doc_ids.index(doc_id) + 1\n",
    "        ranks.append(rank)\n",
    "    else:\n",
    "        errors.append(doc_id)\n",
    "\n",
    "    ap = average_precision_single_relevant(reranked_doc_ids, doc_id)\n",
    "    total_ap += ap\n",
    "\n",
    "     # Usefulness check for 4s and 5s\n",
    "    for doc_id_candidate in reranked_doc_ids:\n",
    "        for _, d in fours_fives.iterrows():\n",
    "            total_fourfive += 1\n",
    "            if doc_id_candidate == d[\"id\"] and d[\"id2\"] not in reranked_doc_ids:\n",
    "                fourfive_count += 1\n",
    "\n",
    "# Results\n",
    "accuracy = counter / total\n",
    "avg_rank = sum(ranks) / len(ranks) if ranks else None\n",
    "map_score = total_ap / total if total > 0 else 0.0\n",
    "usefulness = fourfive_count / total_fourfive if total_fourfive > 0 else 0.0\n",
    "\n",
    "print(f\"\\nFound correct doc in top 5 for {counter}/{total} queries.\")\n",
    "print(f\"Accuracy@5: {accuracy:.2%}\")\n",
    "if avg_rank is not None:\n",
    "    print(f\"Average rank position (for successful hits): {avg_rank:.2f}\")\n",
    "else:\n",
    "    print(\"No correct documents found in top 5; cannot compute average rank.\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")\n",
    "print(f\"Similarity score usefulness: {usefulness:.4f}\")\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df5f6a-edfb-4577-b226-e809c7274f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b3b38-ac69-4cd2-96e6-f3685ea9f48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
