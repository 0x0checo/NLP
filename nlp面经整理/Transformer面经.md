**1.Transformer为何使用多头注意力机制？**

a.多头保证了tansformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。
（多头注意力机制将输入的特征向量分成多个子空间，每个注意力头独立地在这些子空间上计算注意力。这样，每个头可以专注于不同的语义或模式（例如，句法结构、语义关系、局部依赖等）。）

b.适合并行计算，因为每个头的计算是独立的，能显著提高计算效率。

**2.layernorm和batchnorm的区别？为什么Transformer使用layernorm？**

bn:在batch维度上（即对一个 mini-batch 中同一维度的特征做归一化）

ln:在特征维度上（即对单个样本的所有特征做归一化）

why: 

✅ 原因 1：Batch 大小通常较小
NLP 或序列任务中，batch size 可能很小（甚至为 1），BN 在这种情况下统计量不稳定，性能下降。
LN 完全不依赖 batch size，保证一致性。

✅ 原因 2：Transformer 的输入是序列
在 Transformer 中，每个 token 都有一个 embedding 向量，LN 是在 embedding 的维度上做归一化，非常自然。
如果用 BN，会在 batch 内不同句子的 token 之间混合统计量，反而破坏语义。
（想想limu的立方体！！！）

✅ 原因 3：训练/推理一致性
BN 在训练和推理阶段使用的统计量不同（训练时用 mini-batch 统计，推理时用移动平均），可能带来不一致问题。
LN 在训练和推理时行为完全一样，更稳定。

Transformer 使用 LayerNorm，因为它对 batch size 不敏感，更适合序列建模，不会破坏 token 的独立性，而且训练和推理阶段行为一致。

**3.残差连接是什么？**

让输入直接跳过若干层网络，加到输出上。（解决梯度消失/爆炸问题）

**4.什么是掩码注意力？**

在生成任务中，*解码器*在预测第t个词的时候，不应该看到未来（t+1。。）的词，所以要对第t个词后面的位置打上掩码，让模型只看当前和之前的token。

**5.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？**

a.增强表达能力：通过为Q和K使用不同的权重矩阵（$W_Q$ 和 $W_K$），Transformer可以将输入向量（如词嵌入）投影到不同的子空间中。

b.解耦查询和键的角色：在注意力机制中，Q表示当前需要关注的“查询”信息，而K表示用于匹配的“键”信息。它们的角色在语义上是不同的：Q决定了“找什么”，K决定了“被找的内容”。使用不同的权重矩阵允许模型为这两种角色学习专门的表示，从而更精确地建模输入之间的关系。

**6.什么是 Positional Encoding（位置编码）？**

给输入的词向量加上位置信息，让transformer能理解序列的顺序问题。

缺点是，它是固定函数，不能根据任务进行调整；只能表示绝对位置；长序列泛化有限。

**7.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？**

矩阵乘法可以并行运算，效率高，计算速度快。

**8.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解**

点乘的方差随dk增大而增大 → softmax 饱和 → 梯度消失。缩放除以dk的平方根，使打分的方差约为1，softmax稳定。

**9.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？**

避免词向量太小，被位置编码淹没。使得词向量的数值尺度和位置编码在同一尺度，从而保证语义信息和位置信息都能被很好利用。

**10.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？**

优点：逐位置独立，不依赖其它位置，易并行计算；包含线性和非线形计算，可以学习更复杂模式。

缺点：不考虑上下文，参数量大，对激活函数选择敏感。

**11.Encoder端和Decoder端是如何进行交互的？**

Encoder 和 Decoder 的交互 主要发生在 Decoder 的第二个子层：Encoder-Decoder Attention。在这里，Decoder 的 Query 去“询问” Encoder 的上下文表示（Key/Value），从而在生成目标序列时考虑源序列信息。

**12.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)**

在Transformer架构中，"Decoder"阶段的多头自注意力（通常指Decoder中的多头自注意力）与Encoder的多头自注意力有所不同，主要体现在以下几个方面：

1. 关注方向：
   - Encoder的多头自注意力：在输入序列的所有位置之间进行双向关注（即全局关注），帮助编码器捕捉输入的上下文信息。
   - Decoder的多头自注意力：在生成过程中采用的是“部分因果”关注机制，通常用“masked”自注意力，确保每个位置只能关注到当前位置及之前的位置，防止未来信息泄露，保证自回归生成的正确性。

2. 机制差异：
   - Encoder中的自注意力：没有任何遮盖（mask），每个位置都可以看见整个输入序列。
   - Decoder中的自注意力：包含遮盖（mask），阻止当前位置关注未来位置，以确保序列生成的因果关系。

3. 作用目的：
   - Encoder的自注意力：提取输入序列的全局特征，用于后续的编码表示。
   - Decoder的自注意力：在生成每个词时，限制模型只能看见已生成的部分，确保输出的连续性和因果关系。

总结：
- Encoder的多头自注意力关注完整序列，帮助编码输入信息。
- Decoder的多头自注意力使用掩码（mask），只关注已生成部分，确保生成的正确性。

**13.Transformer的并行化体现在哪个地方？Decoder端可以做并行化吗？**

1. 多头自注意力机制的并行计算：
   - 在每个位置的多头注意力计算中，各个头的计算是相互独立的，可以同时进行。同时，序列中不同位置自     - 注意力也可以并行计算，使得整体的注意力计算具有高度并行性。

2. Transformer层内部的并行化：
   - 每个Transformer层中的子模块（多头自注意力、前馈网络）可以并行执行，因为它们是独立的操作。意    - 味着多个Transformer层可以在硬件层面同时进行计算（流水线并行化），或者在单个层内实现GPU多核的并行。

3. 序列长度的处理：
   - 在训练时，为了利用GPU的并行优势，通常会把整个批次中的序列填充到相同长度，并同时输入模型，得     - 每个序列中的每个位置都能同步进行处理。

关于 Decoder端是否可以做并行化：

- 训练时：
Decoder的自注意力部分可以并行化（使用掩码方式），因为每个位置的注意力可以在同步计算中完成（只关注到已生成的部分）。
但在实际实现中，为了保持因果关系，通常会用掩码（mask）限制每个位置只能看到当前及之前位置。结果是自注意力中的掩码操作会引入一定的串行依赖，限制了完全的时间维度上的并行化，但在每个步骤内的计算仍是高度并行的。

- 推理/生成时（解码阶段）：
这是一个序列逐步生成的过程（自回归），每次生成一个词后，才能进行下一步，导致极大程度上串行化，这是Transformer在推理阶段的瓶颈。
 目前的优化方法包括：

- 缓存机制：在每一步保存前一阶段的注意力计算结果，只更新新位置的计算，从而避免重复全部重新计算。
- 并行化批次：可以同时处理多个序列或多个生成任务，实现一定程度的并行。





