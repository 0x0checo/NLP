**1.Transformer为何使用多头注意力机制？**

a.多头保证了tansformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。
（多头注意力机制将输入的特征向量分成多个子空间，每个注意力头独立地在这些子空间上计算注意力。这样，每个头可以专注于不同的语义或模式（例如，句法结构、语义关系、局部依赖等）。）

b.适合并行计算，因为每个头的计算是独立的，能显著提高计算效率。

**2.layernorm和batchnorm的区别？为什么Transformer使用layernorm？**

bn:在batch维度上（即对一个 mini-batch 中同一维度的特征做归一化）

ln:在特征维度上（即对单个样本的所有特征做归一化）

why: 

✅ 原因 1：Batch 大小通常较小
NLP 或序列任务中，batch size 可能很小（甚至为 1），BN 在这种情况下统计量不稳定，性能下降。
LN 完全不依赖 batch size，保证一致性。

✅ 原因 2：Transformer 的输入是序列
在 Transformer 中，每个 token 都有一个 embedding 向量，LN 是在 embedding 的维度上做归一化，非常自然。
如果用 BN，会在 batch 内不同句子的 token 之间混合统计量，反而破坏语义。
（想想limu的立方体！！！）

✅ 原因 3：训练/推理一致性
BN 在训练和推理阶段使用的统计量不同（训练时用 mini-batch 统计，推理时用移动平均），可能带来不一致问题。
LN 在训练和推理时行为完全一样，更稳定。

Transformer 使用 LayerNorm，因为它对 batch size 不敏感，更适合序列建模，不会破坏 token 的独立性，而且训练和推理阶段行为一致。

**3.残差连接是什么？**

让输入直接跳过若干层网络，加到输出上。（解决梯度消失/爆炸问题）

**4.什么是掩码注意力？**

在生成任务中，*解码器*在预测第t个词的时候，不应该看到未来（t+1。。）的词，所以要对第t个词后面的位置打上掩码，让模型只看当前和之前的token。

**5.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？**

a.增强表达能力：通过为Q和K使用不同的权重矩阵（$W_Q$ 和 $W_K$），Transformer可以将输入向量（如词嵌入）投影到不同的子空间中。

b.解耦查询和键的角色：在注意力机制中，Q表示当前需要关注的“查询”信息，而K表示用于匹配的“键”信息。它们的角色在语义上是不同的：Q决定了“找什么”，K决定了“被找的内容”。使用不同的权重矩阵允许模型为这两种角色学习专门的表示，从而更精确地建模输入之间的关系。

**6.什么是 Positional Encoding（位置编码）？**

给输入的词向量加上位置信息。

