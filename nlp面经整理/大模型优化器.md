**1.梯度下降是什么？**

不断沿着函数下降最快的方向（负梯度方向）迭代更新参数，从而逐步逼近最优解。

**2.SGD是如何实现的，有什么优缺点？**

SGD（Stochastic Gradient Descent）是梯度下降算法的随机变体，用于优化神经网络或其他模型的参数。它不像批量梯度下降（BGD）那样用整个数据集计算梯度，
而是每次迭代随机采样一个（或小批量）样本计算梯度并更新参数，从而加速训练并引入噪声以逃离局部最优。

**3.介绍一下Adam**

Adam（Adaptive Moment Estimation，自适应矩估计）是一种高效的随机梯度下降变体，由Diederik P. Kingma和Jimmy Ba于2014年提出。
它结合了Momentum（动量）和RMSprop（自适应学习率）的优点，通过维护梯度的一阶和二阶矩的指数移动平均，来为每个参数动态调整学习率。
Adam已成为深度学习框架（如PyTorch、TensorFlow）的默认优化器，尤其适合噪声大、非平稳目标的优化问题（如CNN、RNN训练）。

**4.批量梯度下降（BGD）、随机梯度下降（SGD）与小批量随机梯度下降（Mini-Batch GD）的区别？**

# 1️⃣ 批量梯度下降（BGD, Batch Gradient Descent）

* **实现**：每次更新参数时，用 **整个训练集** 计算一次梯度。

  $$
  \theta \gets \theta - \eta \, \nabla_\theta \frac{1}{N}\sum_{i=1}^{N} L(x_i, y_i;\theta)
  $$
* **优点**

  * 梯度估计准确，收敛稳定。
* **缺点**

  * 每次迭代需要处理所有样本，**计算代价大**，大数据集上训练很慢。
* **适用情况**

  * 数据量较小（如几千样本）时可用。

---

# 2️⃣ 随机梯度下降（SGD, Stochastic Gradient Descent）

* **实现**：每次只用 **一个样本** 计算梯度更新参数。

  $$
  \theta \gets \theta - \eta \, \nabla_\theta L(x_i, y_i;\theta)
  $$
* **优点**

  * 每次迭代开销小，更新非常快。
  * 引入噪声，能跳出局部最优，更有可能找到全局最优。
* **缺点**

  * 更新方向噪声很大，**收敛过程震荡**，不稳定。
* **适用情况**

  * 大数据集，在线学习场景（流式数据）。

---

# 3️⃣ 小批量随机梯度下降（Mini-Batch Gradient Descent）

* **实现**：每次用一个 **小批量样本（如 32, 64, 128）** 来估计梯度。

  $$
  \theta \gets \theta - \eta \, \nabla_\theta \frac{1}{m}\sum_{i=1}^{m} L(x_i, y_i;\theta)
  $$
* **优点**

  * 综合了 BGD 和 SGD 的优势：既有一定的收敛稳定性，又能高效训练。
  * 方便利用 **GPU 并行计算**。
* **缺点**

  * 需要选择合适的 batch size。
* **适用情况**

  * 深度学习的主流选择（几乎所有神经网络训练）。

---

# 4️⃣ SGD 的改进版优化器

SGD 本身简单，但噪声大、收敛慢。后来提出了一些改进方法，比如 **Momentum、RMSProp、Adam**。

---

## (a) Momentum

* 思想：模拟物理动量，考虑过去梯度的“惯性”。

  $$
  v_t = \beta v_{t-1} + (1-\beta)\nabla_\theta L(\theta_t)
  $$

  $$
  \theta_{t+1} = \theta_t - \eta v_t
  $$
* 优点：加快收敛，减少震荡。

---

## (b) Adam（Adaptive Moment Estimation）

* **思想**：结合了 **Momentum + RMSProp**。

  * 一阶矩估计（梯度的动量）：

    $$
    m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t
    $$
  * 二阶矩估计（梯度平方的指数滑动平均）：

    $$
    v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
    $$
  * 经过偏差修正后：

    $$
    \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}
    $$

* **优点**

  * 自适应学习率（不同参数有不同的学习率）。
  * 收敛快，几乎不需要太多调参。

* **缺点**

  * 有时会陷入非最优解，泛化能力比 SGD 略差。

* **适用情况**

  * 深度学习任务默认优化器（NLP、CV 常用）。

---

# 🔹 总结对比表

| 方法                | 样本量                | 优点           | 缺点            | 常用场景       |
| ----------------- | ------------------ | ------------ | ------------- | ---------- |
| **BGD**           | 全部                 | 收敛稳定         | 计算慢，内存大       | 小数据集       |
| **SGD**           | 1                  | 计算快，可跳出局部最优  | 收敛震荡大         | 在线学习，大数据流式 |
| **Mini-Batch GD** | 一小批                | 稳定+高效，支持 GPU | 需调 batch size | 深度学习主流     |
| **Adam**          | Mini-batch + 自适应更新 | 快速收敛，自适应学习率  | 泛化差，有时不如 SGD  | 大多数深度学习任务  |





