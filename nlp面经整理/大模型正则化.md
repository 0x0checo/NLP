**1.什么是正则化？**

在机器学习和深度学习中，正则化（regularization）主要是为了 防止过拟合。

过拟合：模型在训练集上表现很好，但在测试集或真实数据上效果差。

原因：模型参数太多、自由度太高，它可能“死记硬背”训练数据里的噪声。

正则化通过在损失函数中加入对模型复杂度的惩罚项，让模型 偏好更简单的参数分布（例如小权重），提升泛化能力。

**2.权重衰减的目的是什么？**

权重衰减是 最常用的正则化方法之一，通常就是 L2 正则化。

**3.什么是L1正则化和L2正则化？**

---

### 1. **L1 正则化 (Lasso Regularization)**

公式：

$$
L = L_{original} + \lambda \|w\|_1 = L_{original} + \lambda \sum_i |w_i|
$$

* 惩罚的是 **权重的绝对值之和**。
* **效果：**

  * 会让部分权重收缩到 **完全为 0**。
  * 产生 **稀疏解**（sparse solution），即只保留一部分有用特征，起到 **特征选择** 的作用。
* **直观理解**：

  * L1 像在权重上加了“拉绳”，很多权重会被直接拉到零。

👉 适合用于：当你怀疑很多特征是无关或冗余时（例如高维特征场景）。

---

### 2. **L2 正则化 (Ridge Regularization / Weight Decay)**

公式：

$$
L = L_{original} + \lambda \|w\|_2^2 = L_{original} + \lambda \sum_i w_i^2
$$

* 惩罚的是 **权重的平方和**。
* **效果：**

  * 会让权重整体都 **变小**，但通常不会变成完全的零。
  * 不会主动做特征选择，但能防止权重过大，提高稳定性。
* **直观理解**：

  * L2 像在权重上加了“橡皮筋”，拉住它们不让发散，但不会强行压成零。

👉 适合用于：大多数深度学习任务，稳定训练、防止过拟合。

---

### 3. **对比总结**

| 特性      | L1 正则化        | L2 正则化     |   |            |
| ------- | ------------- | ---------- | - | ---------- |
| 惩罚形式    | (\sum         | w          | ) | $\sum w^2$ |
| 权重效果    | 产生稀疏解，部分权重=0  | 权重整体变小但不为0 |   |            |
| 是否做特征选择 | ✅ 是           | ❌ 否        |   |            |
| 常见应用    | 高维稀疏模型（如文本分类） | 深度学习（权重衰减） |   |            |

---

**4.为何只对权重进行正则惩罚，而不针对偏置？**

这是因为偏置的作用和权重不同：

- 权重𝑤：决定模型对输入特征的依赖程度。过大的权重意味着模型可能对某个特征过拟合。

- 偏置b：只是调整输出的整体平移（shift），不会增加模型复杂度。

如果也正则化偏置：

- 会不必要地限制模型拟合数据的能力。

- 可能导致模型无法找到合适的平移，从而降低性能。

**5.为什么 L1 正则化可以产生稀疏解，而 L2 不会？**

核心原因在于*几何约束形状不同*和*梯度性质不同*。

---

L1 正则化约束:
- 约束集合是一个菱形（diamond）。

- 在和损失函数的等高线相交时，最优解经常落在菱形的尖角上。

- 尖角对应某些维度正好 = 0 → 导致稀疏。

L2 正则化约束：

- 约束集合是一个圆形/球体（circle/sphere）。

- 在和等高线相交时，很难正好落在坐标轴上（即某个权重为 0）。

- 所以解一般是整体缩小，而不是稀疏。

---

L1 惩罚项：

- 对应梯度：λ⋅sign(w)， 当 w 很小时，它仍然给一个固定的推力把 w 压向 0。
- 当 𝑤 = 0时，梯度是一个区间（subgradient in [−λ,λ]），因此模型很容易“停”在零点。

L2 惩罚项：

- 对应梯度：2λw， 当w接近 0 时，梯度也趋近 0。
- 所以不会强迫权重变成精确的零，只是越来越小。


