**1.为什么需要激活函数？**

引入非线性变换，捕捉复杂模式。

**2.常见激活函数汇总**

## 1. **Sigmoid（逻辑函数）**

公式：

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

* **优点**

  * 输出范围 (0,1)，可以当作概率。
  * 历史上常用于二分类任务的输出层。

* **缺点**

  * **梯度消失**：当 |x| 较大时梯度趋近于 0，难以训练深层网络。
  * **非零均值**：输出不以 0 为中心，容易导致梯度更新效率低。

* **适用情况**

  * 现在多用于 **二分类模型的输出层**，不常用于隐藏层。

---

## 2. **Tanh（双曲正切函数）**

公式：

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

* **优点**

  * 输出范围 (-1,1)，比 sigmoid 更“居中”，收敛速度快一些。
  * 在某些循环神经网络（RNN）中常见。

* **缺点**

  * 依然有 **梯度消失**问题（在极端输入下）。

* **适用情况**

  * 适用于一些对输出分布居中有要求的场景，比如 **传统 RNN**。

---

## 3. **ReLU（Rectified Linear Unit）**

公式：

$$
f(x) = \max(0, x)
$$

* **优点**

  * 计算简单，高效。
  * 避免梯度消失（正区间梯度恒为 1）。
  * 收敛速度比 sigmoid/tanh 快。

* **缺点**

  * **Dead ReLU** 问题：当 x<0 时梯度为 0，神经元可能“死亡”不再更新。
  * 输出非零均值，可能影响收敛速度。

* **适用情况**

  * 深度神经网络的默认选择（特别是卷积神经网络 CNN 的隐藏层）。

---

## 4. **Leaky ReLU**

公式：

$$
f(x) = \begin{cases} 
x & x > 0 \\ 
\alpha x & x \leq 0 
\end{cases}
$$

其中 $\alpha$ 是一个小正数（如 0.01）。

* **优点**

  * 解决 ReLU 的“死亡”问题（负区间也有小梯度）。
  * 训练更稳定。

* **缺点**

  * 输出非零均值，理论上仍可能影响收敛。
  * $\alpha$ 需要调参。

* **适用情况**

  * ReLU 的改进版，经常用于深度网络（ResNet、GAN 等）。

---

## 5. **Parametric ReLU (PReLU)**

和 Leaky ReLU 类似，但 $\alpha$ 是可学习参数。

* **优点**

  * 可以根据数据自动学习负区间的斜率。
* **缺点**

  * 增加少量参数，可能带来过拟合风险。
* **适用情况**

  * 在大模型或需要更高表达能力的深度网络中。

---

## 6. **ELU (Exponential Linear Unit)**

公式：

$$
f(x) = \begin{cases} 
x & x > 0 \\ 
\alpha (e^x - 1) & x \leq 0 
\end{cases}
$$

* **优点**

  * 输出有负值，均值更接近 0，加快收敛。
  * 负区间有平滑的非零梯度，比 ReLU 更稳定。

* **缺点**

  * 计算比 ReLU 更复杂。

* **适用情况**

  * 在对训练稳定性要求高的深层网络中。

---

## 7. **Swish**

Google 提出，公式：

$$
f(x) = x \cdot \sigma(\beta x)
$$

* **优点**

  * 平滑、非单调，在深层网络表现优于 ReLU。
  * 兼顾 ReLU 的稀疏性和 Sigmoid 的平滑性。

* **缺点**

  * 计算复杂度高于 ReLU。

* **适用情况**

  * 在大规模深度学习任务（如图像分类、NLP 预训练模型）中，往往比 ReLU 更优。

---

## 8. **Mish**

公式：

$$
f(x) = x \cdot \tanh(\ln(1+e^x))
$$

* **优点**

  * 类似 Swish，比 ReLU 更平滑。
  * 在部分任务（计算机视觉）上性能优于 ReLU/Swish。

* **缺点**

  * 计算更复杂。

* **适用情况**

  * 在需要高精度的 CV 任务（YOLOv4 等曾使用）。

---

## 9. **Softmax**

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

* **优点**

  * 将向量映射为概率分布。
* **缺点**

  * 容易受极值影响。
* **适用情况**

  * 常用于 **多分类任务的输出层**。

---

### 📌 总结选择建议：

* **输出层**

  * 二分类 → Sigmoid
  * 多分类 → Softmax
* **隐藏层**

  * 默认：ReLU（快速简单）
  * 避免 Dead ReLU：Leaky ReLU / PReLU
  * 更稳定、零均值：ELU
  * 高性能、大模型：Swish / Mish



